{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing is the capital of  it.\n",
      "Beijing is the capital of  India.\n",
      "Beijing is the capital of  China.\n",
      "=== Tokenization ===\n",
      "['<s>', 'Be', 'ijing', ' is', ' the', ' capital', ' of', '<mask>', '.', '</s>']\n",
      "=== Matrix Sizes ===\n",
      "z_cls: torch.Size([768])\n",
      "z_i: torch.Size([7, 768])\n",
      "U: torch.Size([768, 768])\n",
      "=== Attention ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:rgba(255, 0, 0, 6.087010783131223e-18)\">Be</span> <span style=\"background-color:rgba(255, 0, 0, 6.422041599272706e-15)\">ijing</span> <span style=\"background-color:rgba(255, 0, 0, 2.495540232794191e-12)\"> is</span> <span style=\"background-color:rgba(255, 0, 0, 0.05572349950671196)\"> the</span> <span style=\"background-color:rgba(255, 0, 0, 0.9442765116691589)\"> capital</span> <span style=\"background-color:rgba(255, 0, 0, 6.056448880820478e-14)\"> of</span> <span style=\"background-color:rgba(255, 0, 0, 4.214438504614869e-24)\">.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "access_token = \"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\"\n",
    "text = \"Beijing is the capital of <mask>.\" \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"JackBAI/crate-base\", token=access_token)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"JackBAI/crate-base\", token=access_token)\n",
    "logits = model(**inputs).logits\n",
    "mask_token_logits = logits[0, mask_token_index, :]\n",
    "\n",
    "top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_3_tokens:\n",
    "    print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))\n",
    "    \n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "input_id = inputs.input_ids\n",
    "print(\"=== Tokenization ===\")\n",
    "encoded = []\n",
    "mask_id = 0\n",
    "for i in range(input_id.shape[1]):\n",
    "    encoded_item = tokenizer.decode(input_id[0,i].item())\n",
    "    encoded.append(encoded_item)\n",
    "    if encoded_item == '<mask>':\n",
    "        mask_id = i\n",
    "print(encoded)\n",
    "\n",
    "# Pass the input through the model to get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "print(f\"=== Matrix Sizes ===\")\n",
    "hidden_states = outputs.hidden_states\n",
    "embedding_output = hidden_states[0]\n",
    "last_hidden_states = hidden_states[-1] # [1, 8, 768]\n",
    "\n",
    "# get last layer [CLS] (<s>) vector\n",
    "z_cls = last_hidden_states[0,mask_id,:]\n",
    "print(\"z_cls:\", z_cls.shape) # [768]\n",
    "\n",
    "# get last layer matrix input\n",
    "z_i = last_hidden_states[0,1:-1,:]\n",
    "# exclude the state at mask_id by setting it to -inf\n",
    "z_i = torch.cat((z_i[:mask_id-1, :], z_i[mask_id:, :]))\n",
    "print(\"z_i:\", z_i.shape) # [9, 768]\n",
    "\n",
    "# get last layer model weights\n",
    "U = model.crate.encoder.layer[-1].attention.self.qkv.weight.data\n",
    "print(\"U:\", U.shape) # [768, 768]\n",
    "\n",
    "Uz_i = torch.matmul(U, z_i.T) # [768, 9]\n",
    "Uz_cls = torch.matmul(U, z_cls) # [768]\n",
    "\n",
    "# calculate per-token attention, i.e., each column of Uz_i dot product by Uz_cls\n",
    "attentions = torch.matmul(Uz_i.T, Uz_cls) # [9]\n",
    "    \n",
    "from IPython.display import HTML\n",
    "\n",
    "# Define a function to create HTML with background color based on attention score\n",
    "def colored_text_with_attention(tokens, attentions):\n",
    "    html = \"\"\n",
    "    for token, attention in zip(tokens, attentions):\n",
    "        color = f'rgba(255, 0, 0, {attention})'  # Adjust the color as per your preference\n",
    "        html += f'<span style=\"background-color:{color}\">{token}</span> '\n",
    "    return HTML(html)\n",
    "\n",
    "print(\"=== Attention ===\")\n",
    "attentions = torch.softmax(attentions, dim=0)\n",
    "tokens = encoded[1:-1]\n",
    "tokens = tokens[:mask_id-1] + tokens[mask_id:]\n",
    "colored_text_with_attention(tokens, attentions.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
